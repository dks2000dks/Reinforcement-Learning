{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIBg6Wb5wNn9"
   },
   "source": [
    "# Dino-Game\n",
    "\n",
    "Playing Dino-Game of Chrome using Reinforcement Learning Algorithm.\n",
    "\n",
    "Inspiration of Idea and Source of Learning:\n",
    "https://blog.paperspace.com/dino-run/\n",
    "\n",
    "#### Key Points:\n",
    "- Selenium is used to Interface Deep Learning Model and Browser.\n",
    "- Open-CV is used to Pre-Process Images\n",
    "- TensorFlow for building Deep Learning Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGKxWzZdwbQV"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvtbXA4Mwdwa"
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnHKG9E9wxsv"
   },
   "source": [
    "Installing selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "uNVrNf1Iw09E",
    "outputId": "22bc99f5-0131-43ba-a314-c3c15414c673"
   },
   "outputs": [],
   "source": [
    "!pip3 install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMSkRU_Bwcnz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import io\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from random import randint\n",
    "import os\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import BatchNormalization,Conv2D,Conv2DTranspose,LeakyReLU,Activation,Flatten,Dense,Reshape,Input,Concatenate,MaxPooling2D\n",
    "from tensorflow.keras.initializers import orthogonal\n",
    "from tensorflow.keras.models import Model,load_model\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nisLehqXiqwc"
   },
   "source": [
    "## Global Variables to keep track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ALT7VIcYivuD"
   },
   "outputs": [],
   "source": [
    "game_url = 'chrome://dino'\n",
    "chrome_driver_path = \"chromedriver_linux64/chromedriver\"\n",
    "loss_filepath = \"Objects/loss_dataframe.csv\"\n",
    "actions_filepath = \"Objects/actions_dataframe.csv\"\n",
    "Qvalues_filepath = \"Objects/Qvalues_dataframe.csv\"\n",
    "scores_filepath = \"Objects/scores_dataframe.csv\"\n",
    "\n",
    "loss_df = pd.read_csv(loss_filepath) if os.path.isfile(loss_filepath) else pd.DataFrame(columns =['loss'])\n",
    "scores_df = pd.read_csv(scores_filepath) if os.path.isfile(loss_filepath) else pd.DataFrame(columns = ['scores'])\n",
    "actions_df = pd.read_csv(actions_filepath) if os.path.isfile(actions_filepath) else pd.DataFrame(columns = ['actions'])\n",
    "Qvalues_df =pd.read_csv(Qvalues_filepath) if os.path.isfile(Qvalues_filepath) else pd.DataFrame(columns = ['Qvalues'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZvWlLGc1SHH"
   },
   "source": [
    "## Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0VvGC-L51U1S"
   },
   "outputs": [],
   "source": [
    "#create id for canvas for faster selection from DOM\n",
    "init_script = \"document.getElementsByClassName('runner-canvas')[0].id = 'runner-canvas'\"\n",
    "\n",
    "#get image from canvas\n",
    "getbase64Script = \"canvasRunner = document.getElementById('runner-canvas'); \\\n",
    "return canvasRunner.toDataURL().substring(22)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y910ORHHfAFN"
   },
   "source": [
    "## Game Module\n",
    "\n",
    "Interfacing between JavaScript and Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQvMJmPReOqE"
   },
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self,custom_config=True):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"disable-infobars\")\n",
    "        chrome_options.add_argument(\"--mute-audio\")\n",
    "        self._driver = webdriver.Chrome(executable_path = chrome_driver_path)\n",
    "        self._driver.set_window_position(x=-10,y=0)\n",
    "        try:\n",
    "            self._driver.get(game_url)\n",
    "        except:\n",
    "            pass\n",
    "        if custom_config:\n",
    "            self._driver.execute_script(\"Runner.config.ACCELERATION=0\")\n",
    "        self._driver.execute_script(init_script)\n",
    "    def get_playing(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.playing\")\n",
    "    def get_crashed(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.crashed\")\n",
    "    def restart(self):\n",
    "        self._driver.execute_script(\"Runner.instance_.restart()\")\n",
    "    def press_up(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ARROW_UP)\n",
    "    def press_down(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ARROW_DOWN)\n",
    "    def get_score(self):\n",
    "        score_array = self._driver.execute_script(\"return Runner.instance_.distanceMeter.digits\")\n",
    "        score = ''.join(score_array)\n",
    "        return int(score)\n",
    "    def pause(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.stop()\")\n",
    "    def resume(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.play()\")\n",
    "    def end(self):\n",
    "        return self._driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mxl31b9zgp95"
   },
   "source": [
    "## DinoAgent\n",
    "\n",
    "Actions of DinoAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZb8IyfCgnHR"
   },
   "outputs": [],
   "source": [
    "class DinoAgent:\n",
    "    def __init__(self,game):\n",
    "        self._game = game\n",
    "        self.jump()\n",
    "    def is_running(self):\n",
    "        return self._game.get_playing()\n",
    "    def is_crashed(self):\n",
    "        return self._game.get_crashed()\n",
    "    def jump(self):\n",
    "        self._game.press_up()\n",
    "    def duck(self):\n",
    "        self._game.press_down()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8lsN291hvVA"
   },
   "source": [
    "## Game State Module or Environment for Agent\n",
    "\n",
    "This Module send actions to agents and changes state of Environment as per action. It decides the Reward for the Agent and returns Experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijnf5yIghnh8"
   },
   "outputs": [],
   "source": [
    "class Game_State:\n",
    "    def __init__(self,agent,game):\n",
    "        self._agent = agent\n",
    "        self._game = game\n",
    "        self._display = ShowImage()\n",
    "        self._display.__next__()\n",
    "    def get_state(self,action):\n",
    "        score = self._game.get_score()\n",
    "        # Assigning Rewards Dynamically\n",
    "        reward = 0.01 * score\n",
    "        isOver = False\n",
    "\n",
    "        if action[1] == 1:\n",
    "            actions_df.loc[len(actions_df)] = action[1]\n",
    "            self._agent.jump()\n",
    "\n",
    "        elif action[2] == 1:\n",
    "            actions_df.loc[len(actions_df)] = action[2]\n",
    "            self._agent.duck()\n",
    "\n",
    "        image = grab_screen(self._game._driver) \n",
    "        #self._display.send(image)\n",
    "       \n",
    "        if self._agent.is_crashed():\n",
    "            scores_df.loc[len(scores_df)] = score\n",
    "            self._game.restart()\n",
    "            # Assigning Rewards Dynamically\n",
    "            reward = -11/score\n",
    "            isOver = True\n",
    "            \n",
    "        return image, reward, isOver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Kt9teyi0xb-"
   },
   "source": [
    "## Extracting Image and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gDzMFE1hmzPy"
   },
   "outputs": [],
   "source": [
    "def PreProcessImage(Image):\n",
    "    img = cv2.cvtColor(Image, cv2.COLOR_BGR2GRAY)\n",
    "    img = img[:300,:500]\n",
    "    img = cv2.Canny(img, threshold1 = 100, threshold2 = 200)\n",
    "    img = cv2.resize(img,(80,80))\n",
    "    return img\n",
    "    \n",
    "def grab_screen(_driver):\n",
    "    image_b64 = _driver.execute_script(getbase64Script)\n",
    "    screen = np.array(Image.open(BytesIO(base64.b64decode(image_b64))))\n",
    "    image = PreProcessImage(screen)\n",
    "    return image\n",
    "\n",
    "def ShowImage(graphs = False):\n",
    "    while True:\n",
    "        screen  = (yield)\n",
    "        window_title = \"Logs\" if graphs else \"Game_Play\"\n",
    "        cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)\n",
    "        imageScreen = cv2.resize(screen, (800,400))\n",
    "        cv2.imshow(window_title, screen)\n",
    "        if (cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxV3LFh9S11h"
   },
   "source": [
    "## Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFGmltYlS3oc"
   },
   "outputs": [],
   "source": [
    "def SaveObject(obj, name):\n",
    "    with open('Objects/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def LoadObject(name):\n",
    "    with open('Objects/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eFzcHtF9G-S"
   },
   "source": [
    "## Parameters\n",
    "\n",
    "**Actions:**\n",
    "- No Jump or Duck\n",
    "- Jump\n",
    "- Duck\n",
    "\n",
    "**Epsilon:**\n",
    "Epsilon is used when we are selecting specific actions base on the Q values we already have. As an example if we select pure greedy method ( epsilon = 0 ) then we are always selecting the highest q value among the all the q values for a specific state. This causes issue in exploration as we can get stuck easily at a local optima.\n",
    "\n",
    "**Gamma:** Decay Rate of Observations\\\n",
    "**Observations:** Time Steps before Training\\\n",
    "**Explore:** Frames over which to Anneal Epilson\\\n",
    "**Initial_Epsilon:** Initial value of Epsilon\\\n",
    "**Final_Epsilon:** Final value of Epsilon\\\n",
    "**Replay_Memeory:** Transitions to Remember\\\n",
    "**Frame_per_Action:** No.of Frames for Action\n",
    "\n",
    "Image Dimensions are (80,80,4) as 4-Frames are Stacked Together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2a-mP3S9Iik"
   },
   "outputs": [],
   "source": [
    "Actions = 3\n",
    "Gamma = 0.99\n",
    "Observation = 100\n",
    "Explore = 20000\n",
    "Final_Epsilon = 0.0001\n",
    "Initial_Epsilon = 0.1\n",
    "Replay_Memory = 10000\n",
    "Batch_Size = 32\n",
    "Frame_per_Action = 1\n",
    "LearningRate = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsTjIhtZTBCp"
   },
   "source": [
    "### Initialise Cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "338ARGmYTFiM"
   },
   "outputs": [],
   "source": [
    "def init_cache():\n",
    "    SaveObject(Initial_Epsilon,\"Epsilon\")\n",
    "    t = 0\n",
    "    SaveObject(t,\"Time\")\n",
    "    D = deque()\n",
    "    SaveObject(D,\"D\")\n",
    "\n",
    "# Initilise Cache only once\n",
    "init_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnDhQwU18AGo"
   },
   "source": [
    "## Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpUWLxo63IBO"
   },
   "outputs": [],
   "source": [
    "def BuildModel():\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), padding='same',strides=(4, 4),input_shape=(80,80,4)))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (4, 4),strides=(2, 2),  padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3),strides=(1, 1),  padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(Actions))\n",
    "    adam = Adam(lr=LearningRate)\n",
    "    model.compile(loss='mse',optimizer=adam)\n",
    "    \n",
    "    if not os.path.isfile(os.getcwd() + '/Model/RLModel.h5'):\n",
    "        print ('Weights Saved')\n",
    "        model.save('Model/RLModel.h5')\n",
    "    \n",
    "    return model\n",
    "\n",
    "RLModel = BuildModel()\n",
    "RLModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9kV1yKJMH5Z"
   },
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "brPcvD0fBK1h"
   },
   "outputs": [],
   "source": [
    "def TrainModel(model,Game_State,ObservePerformance):\n",
    "    lastTime = time.time()\n",
    "    D = LoadObject(\"D\")\n",
    "\n",
    "    Action = np.zeros([Actions])\n",
    "    Action[0] = 1\n",
    "\n",
    "    xt, r0, termination = Game_State.get_state(Action)\n",
    "\n",
    "    st = np.stack((xt, xt, xt, xt), axis=2)\n",
    "    st = st.reshape(1,st.shape[0],st.shape[1],st.shape[2])\n",
    "    Inital_State = st\n",
    "\n",
    "    if ObservePerformance:\n",
    "        Observe = 999999999\n",
    "        epsilon = Final_Epsilon\n",
    "        model.load_weights('Model/RLModel.h5')\n",
    "        model.compile(loss='mse',optimizer=Adam(learning_rate=LearningRate))\n",
    "        print (\"Weights of Model are Loaded\")\n",
    "    else:\n",
    "        Observe = Observation\n",
    "        epsilon = LoadObject(\"Epsilon\")\n",
    "        model.load_weights('Model/RLModel.h5')\n",
    "        model.compile(loss='mse',optimizer=Adam(learning_rate=LearningRate))\n",
    "\n",
    "    t = LoadObject('Time')\n",
    "\n",
    "    while(True):\n",
    "        loss = 0\n",
    "        Q_sa = 0\n",
    "        ActionIndex = 0\n",
    "        rt = 0\n",
    "        at = np.zeros([Actions])\n",
    "\n",
    "        if t%Frame_per_Action == 0:\n",
    "            # Exploring a Action Randomly\n",
    "            if random.random() <= epsilon:\n",
    "                print (\"Performing Random Action\")\n",
    "                ActionIndex = random.randrange(Actions)\n",
    "                at[ActionIndex] = 1\n",
    "            else:\n",
    "                q = model.predict(st)\n",
    "                # Index with Maximum Value is ActionIndex\n",
    "                ActionIndex = maxQ = np.argmax(q)\n",
    "                at[ActionIndex] = 1\n",
    "\n",
    "        if epsilon > Final_Epsilon and t > Observe:\n",
    "            epsilon -= (Initial_Epsilon - Final_Epsilon)/(Explore)\n",
    "\n",
    "        xt1, rt, termination = Game_State.get_state(at)\n",
    "        print ('fps: {0}'.format(1 / (time.time() - lastTime)))\n",
    "\n",
    "        LastTime = time.time()\n",
    "\n",
    "        xt1 = xt1.reshape(1,xt1.shape[0],xt1.shape[1],1)\n",
    "        st1 = np.append(xt1,st[:,:,:,:3],axis=3)\n",
    "\n",
    "        D.append((st,ActionIndex,rt,st1,termination))\n",
    "\n",
    "        if len(D) > Replay_Memory:\n",
    "            D.popleft()\n",
    "\n",
    "        #Training after Observation\n",
    "        if t > Observe:\n",
    "            miniBatch = random.sample(D,Batch_Size)\n",
    "            inputs = np.zeros((Batch_Size,80,80,4))\n",
    "            targets = np.zeros((Batch_Size,Actions))\n",
    "\n",
    "            for i in range(len(miniBatch)):\n",
    "                state_t, action_t, reward_t,state_t1, termination = miniBatch[i]\n",
    "\n",
    "                inputs[i:i+1] = state_t\n",
    "                targets[i] = model.predict(state_t)\n",
    "\n",
    "                Q_sa = model.predict(state_t1)\n",
    "\n",
    "                if termination:\n",
    "                    targets[i:action_t] = reward_t\n",
    "                else:\n",
    "                    targets[i:action_t] = reward_t + Gamma * np.max(Q_sa)\n",
    "\n",
    "            loss += model.train_on_batch(inputs,targets)\n",
    "            loss_df.loc[len(loss_df)] = loss\n",
    "            Qvalues_df.loc[len(loss_df)] = np.max(Q_sa)\n",
    "\n",
    "        st = Inital_State if termination else st1\n",
    "        t += 1\n",
    "\n",
    "        if t%1000 == 0:\n",
    "            Game_State._game.pause()\n",
    "            model.save_weights('Model/RLModel.h5',overwrite=True)\n",
    "            SaveObject(D,\"D\")\n",
    "            SaveObject(t,\"Time\")\n",
    "            SaveObject(epsilon,\"Epsilon\")\n",
    "\n",
    "            loss_df.to_csv(\"./Objects/loss_dataframe.csv\",index=False)\n",
    "            scores_df.to_csv(\"./Objects/scores_dataframe.csv\",index=False)\n",
    "            actions_df.to_csv(\"./Objects/actions_dataframe.csv\",index=False)\n",
    "            Qvalues_df.to_csv(\"./Objects/Qvalues_dataframe.csv\",index=False)\n",
    "\n",
    "            clear_output()\n",
    "            print ('Model Weights Saved')\n",
    "            Game_State._game.resume()\n",
    "\n",
    "        state = \"\"\n",
    "        if t<= Observe:\n",
    "            state = \"Observing\"\n",
    "        elif t>Observe and t<=Observe+Explore:\n",
    "            state = 'Exploring'\n",
    "        else:\n",
    "            state = 'Training'\n",
    "\n",
    "        print (\"Time-Step:\",t, \"/ State\", state)\n",
    "    print (\"Episode Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxLeeUsWOW-c"
   },
   "outputs": [],
   "source": [
    "def playGame(ObservePerformance):\n",
    "    game = Game()\n",
    "    dino = DinoAgent(game)\n",
    "    game_state = Game_State(dino,game)    \n",
    "    model = BuildModel()\n",
    "    try:\n",
    "        TrainModel(model,game_state,ObservePerformance)\n",
    "    except StopIteration:\n",
    "        game.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "w-eJG5Dzasv9",
    "outputId": "35ba6b08-f373-49c4-89b4-5e713aca002a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "playGame(False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Dino-Game.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
