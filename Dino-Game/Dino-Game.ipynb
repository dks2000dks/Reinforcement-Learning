{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIBg6Wb5wNn9"
   },
   "source": [
    "# Dino-Game\n",
    "\n",
    "Playing Dino-Game of Chrome using Reinforcement Learning Algorithm.\n",
    "\n",
    "Inspiration of Idea and Source of Learning:\n",
    "https://blog.paperspace.com/dino-run/\n",
    "\n",
    "#### Key Points:\n",
    "- Selenium is used to Interface Deep Learning Model and Browser.\n",
    "- Open-CV is used to Pre-Process Images\n",
    "- TensorFlow for building Deep Learning Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGKxWzZdwbQV"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvtbXA4Mwdwa"
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnHKG9E9wxsv"
   },
   "source": [
    "Installing selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "uNVrNf1Iw09E",
    "outputId": "22bc99f5-0131-43ba-a314-c3c15414c673"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in /home/krishna/.local/lib/python3.6/site-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in /home/krishna/.local/lib/python3.6/site-packages (from selenium) (1.25.9)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uMSkRU_Bwcnz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import io\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from random import randint\n",
    "import os\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import BatchNormalization,Conv2D,Conv2DTranspose,LeakyReLU,Activation,Flatten,Dense,Reshape,Input,Concatenate,MaxPooling2D\n",
    "from tensorflow.keras.initializers import orthogonal\n",
    "from tensorflow.keras.models import Model,load_model\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "import base64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nisLehqXiqwc"
   },
   "source": [
    "## Global Variables to keep track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ALT7VIcYivuD"
   },
   "outputs": [],
   "source": [
    "game_url = 'chrome://dino'\n",
    "chrome_driver_path = \"chromedriver_linux64/chromedriver\"\n",
    "loss_filepath = \"Objects/loss_dataframe.csv\"\n",
    "actions_filepath = \"Objects/actions_dataframe.csv\"\n",
    "Qvalues_filepath = \"Objects/Qvalues_dataframe.csv\"\n",
    "scores_filepath = \"Objects/scores_dataframe.csv\"\n",
    "\n",
    "loss_df = pd.read_csv(loss_filepath) if os.path.isfile(loss_filepath) else pd.DataFrame(columns =['loss'])\n",
    "scores_df = pd.read_csv(scores_filepath) if os.path.isfile(loss_filepath) else pd.DataFrame(columns = ['scores'])\n",
    "actions_df = pd.read_csv(actions_filepath) if os.path.isfile(actions_filepath) else pd.DataFrame(columns = ['actions'])\n",
    "Qvalues_df =pd.read_csv(Qvalues_filepath) if os.path.isfile(Qvalues_filepath) else pd.DataFrame(columns = ['Qvalues'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZvWlLGc1SHH"
   },
   "source": [
    "## Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0VvGC-L51U1S"
   },
   "outputs": [],
   "source": [
    "#create id for canvas for faster selection from DOM\n",
    "init_script = \"document.getElementsByClassName('runner-canvas')[0].id = 'runner-canvas'\"\n",
    "\n",
    "#get image from canvas\n",
    "getbase64Script = \"canvasRunner = document.getElementById('runner-canvas'); \\\n",
    "return canvasRunner.toDataURL().substring(22)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y910ORHHfAFN"
   },
   "source": [
    "## Game Module\n",
    "\n",
    "Interfacing between JavaScript and Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zQvMJmPReOqE"
   },
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"disable-infobars\")\n",
    "        chrome_options.add_argument(\"--mute-audio\")\n",
    "        self._driver = webdriver.Chrome(executable_path = chrome_driver_path)\n",
    "        self._driver.set_window_position(x=-10,y=0)\n",
    "        try:\n",
    "            self._driver.get(game_url)\n",
    "        except:\n",
    "            pass\n",
    "        self._driver.execute_script(\"Runner.config.ACCELERATION=0\")\n",
    "        self._driver.execute_script(init_script)\n",
    "    def get_playing(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.playing\")\n",
    "    def get_crashed(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.crashed\")\n",
    "    def restart(self):\n",
    "        self._driver.execute_script(\"Runner.instance_.restart()\")\n",
    "    def press_up(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ARROW_UP)\n",
    "    def press_down(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ARROW_DOWN)\n",
    "    def get_score(self):\n",
    "        score_array = self._driver.execute_script(\"return Runner.instance_.distanceMeter.digits\")\n",
    "        score = ''.join(score_array)\n",
    "        return int(score)\n",
    "    def pause(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.stop()\")\n",
    "    def resume(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.play()\")\n",
    "    def end(self):\n",
    "        return self._driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mxl31b9zgp95"
   },
   "source": [
    "## DinoAgent\n",
    "\n",
    "Actions of DinoAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BZb8IyfCgnHR"
   },
   "outputs": [],
   "source": [
    "class DinoAgent:\n",
    "    def __init__(self,game):\n",
    "        self._game = game\n",
    "        self.jump()\n",
    "    def is_running(self):\n",
    "        return self._game.get_playing()\n",
    "    def is_crashed(self):\n",
    "        return self._game.get_crashed()\n",
    "    def jump(self):\n",
    "        self._game.press_up()\n",
    "    def duck(self):\n",
    "        self._game.press_down()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8lsN291hvVA"
   },
   "source": [
    "## Game State Module or Environment for Agent\n",
    "\n",
    "This Module send actions to agents and changes state of Environment as per action. It decides the Reward for the Agent and returns Experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ijnf5yIghnh8"
   },
   "outputs": [],
   "source": [
    "class Game_State:\n",
    "    def __init__(self,agent,game):\n",
    "        self._agent = agent\n",
    "        self._game = game\n",
    "        self._display = ShowImage()\n",
    "        self._display.__next__()\n",
    "    def get_state(self,action):\n",
    "        score = self._game.get_score() \n",
    "        reward = 0.2\n",
    "        isOver = False\n",
    "\n",
    "        if action[1] == 1:\n",
    "            actions_df.loc[len(actions_df)] = action[1]\n",
    "            self._agent.jump()\n",
    "\n",
    "        elif action[2] == 1:\n",
    "            actions_df.loc[len(actions_df)] = action[2]\n",
    "            self._agent.duck()\n",
    "\n",
    "        image = grab_screen(self._game._driver) \n",
    "        #self._display.send(image)\n",
    "       \n",
    "        if self._agent.is_crashed():\n",
    "            scores_df.loc[len(scores_df)] = score\n",
    "            self._game.restart()\n",
    "            reward = -1\n",
    "            isOver = True\n",
    "            \n",
    "        return image, reward, isOver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Kt9teyi0xb-"
   },
   "source": [
    "## Extracting Image and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gDzMFE1hmzPy"
   },
   "outputs": [],
   "source": [
    "def PreProcessImage(Image):\n",
    "    img = cv2.cvtColor(Image, cv2.COLOR_BGR2GRAY)\n",
    "    img = img[:300,:500]\n",
    "    img = cv2.resize(img,(80,80))\n",
    "    return img\n",
    "    \n",
    "def grab_screen(_driver):\n",
    "    image_b64 = _driver.execute_script(getbase64Script)\n",
    "    screen = np.array(Image.open(BytesIO(base64.b64decode(image_b64))))\n",
    "    image = PreProcessImage(screen)#processing image as required\n",
    "    return image\n",
    "\n",
    "def ShowImage(graphs = False):\n",
    "    while True:\n",
    "        screen  = (yield)\n",
    "        window_title = \"Logs\" if graphs else \"Game_Play\"\n",
    "        cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)\n",
    "        imageScreen = cv2.resize(screen, (800,400))\n",
    "        cv2.imshow(window_title, screen)\n",
    "        if (cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxV3LFh9S11h"
   },
   "source": [
    "## Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "TFGmltYlS3oc"
   },
   "outputs": [],
   "source": [
    "def SaveObject(obj, name):\n",
    "    with open('Objects/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def LoadObject(name):\n",
    "    with open('Objects/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eFzcHtF9G-S"
   },
   "source": [
    "## Parameters\n",
    "\n",
    "**Actions:**\n",
    "- No Jump or Duck\n",
    "- Jump\n",
    "- Duck\n",
    "\n",
    "**Epsilon:**\n",
    "Epsilon is used when we are selecting specific actions base on the Q values we already have. As an example if we select pure greedy method ( epsilon = 0 ) then we are always selecting the highest q value among the all the q values for a specific state. This causes issue in exploration as we can get stuck easily at a local optima.\n",
    "\n",
    "**Gamma:** Decay Rate of Observations\\\n",
    "**Observations:** Time Steps before Training\\\n",
    "**Explore:** Frames over which to Anneal Epilson\\\n",
    "**Initial_Epsilon:** Initial value of Epsilon\\\n",
    "**Final_Epsilon:** Final value of Epsilon\\\n",
    "**Replay_Memeory:** Transitions to Remember\\\n",
    "**Frame_per_Action:** No.of Frames for Action\n",
    "\n",
    "Image Dimensions are (80,80,4) as 4-Frames are Stacked Together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "R2a-mP3S9Iik"
   },
   "outputs": [],
   "source": [
    "Actions = 3\n",
    "Gamma = 0.99\n",
    "Observation = 100\n",
    "Explore = 1000\n",
    "Final_Epsilon = 0.0001\n",
    "Initial_Epsilon = 0.1\n",
    "Replay_Memory = 50000\n",
    "Batch_Size = 32\n",
    "Frame_per_Action = 1\n",
    "LearningRate = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsTjIhtZTBCp"
   },
   "source": [
    "### Initialise Cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "338ARGmYTFiM"
   },
   "outputs": [],
   "source": [
    "def init_cache():\n",
    "    SaveObject(Initial_Epsilon,\"Epsilon\")\n",
    "    t = 0\n",
    "    SaveObject(t,\"Time\")\n",
    "    D = deque()\n",
    "    SaveObject(D,\"D\")\n",
    "\n",
    "# Initilise Cache only once\n",
    "init_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnDhQwU18AGo"
   },
   "source": [
    "## Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zpUWLxo63IBO"
   },
   "outputs": [],
   "source": [
    "def BuildModel():\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), padding='same',strides=(4, 4),input_shape=(80,80,4)))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (4, 4),strides=(2, 2),  padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3),strides=(1, 1),  padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(Actions))\n",
    "    adam = Adam(lr=LearningRate)\n",
    "    model.compile(loss='mse',optimizer=adam)\n",
    "    \n",
    "    if not os.path.isfile(os.getcwd() + '/Model/RLModel.h5'):\n",
    "        print ('Weights Saved')\n",
    "        model.save('Model/RLModel.h5')\n",
    "    \n",
    "    return model\n",
    "\n",
    "RLModel = BuildModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9kV1yKJMH5Z"
   },
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "brPcvD0fBK1h"
   },
   "outputs": [],
   "source": [
    "def TrainModel(model,Game_State,ObservePerformance):\n",
    "    lastTime = time.time()\n",
    "    D = LoadObject(\"D\")\n",
    "\n",
    "    Action = np.zeros([Actions])\n",
    "    Action[0] = 1\n",
    "\n",
    "    xt, r0, termination = Game_State.get_state(Action)\n",
    "\n",
    "    st = np.stack((xt, xt, xt, xt), axis=2)\n",
    "    st = st.reshape(1,st.shape[0],st.shape[1],st.shape[2])\n",
    "    Inital_State = st\n",
    "\n",
    "    if ObservePerformance:\n",
    "        Observe = 999999999\n",
    "        epsilon = Final_Epsilon\n",
    "        model.load_weights('Model/RLModel.h5')\n",
    "        model.compile(loss='mse',optimizer=Adam(learning_rate=LearningRate))\n",
    "        print (\"Weights of Model are Loaded\")\n",
    "    else:\n",
    "        Observe = Observation\n",
    "        epsilon = LoadObject(\"Epsilon\")\n",
    "        model.load_weights('Model/RLModel.h5')\n",
    "        model.compile(loss='mse',optimizer=Adam(learning_rate=LearningRate))\n",
    "\n",
    "    t = LoadObject('Time')\n",
    "\n",
    "    while(True):\n",
    "        loss = 0\n",
    "        Q_sa = 0\n",
    "        ActionIndex = 0\n",
    "        rt = 0\n",
    "        at = np.zeros([Actions])\n",
    "\n",
    "        if t%Frame_per_Action == 0:\n",
    "            # Exploring a Action Randomly\n",
    "            if random.random() <= epsilon:\n",
    "                print (\"Performing Random Action\")\n",
    "                ActionIndex = random.randrange(Actions)\n",
    "                at[ActionIndex] = 1\n",
    "            else:\n",
    "                q = model.predict(st)\n",
    "                # Index with Maximum Value is ActionIndex\n",
    "                ActionIndex = maxQ = np.argmax(q)\n",
    "                at[ActionIndex] = 1\n",
    "\n",
    "        if epsilon > Final_Epsilon and t > Observe:\n",
    "            epsilon -= (Initial_Epsilon - Final_Epsilon)/(Explore)\n",
    "\n",
    "        xt1, rt, termination = Game_State.get_state(at)\n",
    "        print ('fps: {0}'.format(1 / (time.time() - lastTime)))\n",
    "\n",
    "        LastTime = time.time()\n",
    "\n",
    "        xt1 = xt1.reshape(1,xt1.shape[0],xt1.shape[1],1)\n",
    "        st1 = np.append(xt1,st[:,:,:,:3],axis=3)\n",
    "\n",
    "        D.append((st,ActionIndex,rt,st1,termination))\n",
    "\n",
    "        if len(D) > Replay_Memory:\n",
    "            D.popleft()\n",
    "\n",
    "        #Training after Observation\n",
    "        if t > Observe:\n",
    "            miniBatch = random.sample(D,Batch_Size)\n",
    "            inputs = np.zeros((Batch_Size,80,80,4))\n",
    "            targets = np.zeros((Batch_Size,Actions))\n",
    "\n",
    "            for i in range(len(miniBatch)):\n",
    "                state_t, action_t, reward_t,state_t1, termination = miniBatch[i]\n",
    "\n",
    "                inputs[i:i+1] = state_t\n",
    "                targets[i] = model.predict(state_t)\n",
    "\n",
    "                Q_sa = model.predict(state_t1)\n",
    "\n",
    "                if termination:\n",
    "                    targets[i:action_t] = reward_t\n",
    "                else:\n",
    "                    targets[i:action_t] = reward_t + Gamma * np.max(Q_sa)\n",
    "\n",
    "            loss += model.train_on_batch(inputs,targets)\n",
    "            loss_df.loc[len(loss_df)] = loss\n",
    "            Qvalues_df.loc[len(loss_df)] = np.max(Q_sa)\n",
    "\n",
    "        st = Inital_State if termination else st1\n",
    "        t += 1\n",
    "\n",
    "        if t%1000 == 0:\n",
    "            Game_State._game.pause()\n",
    "            model.save_weights('RLModel.h5',overwrite=True)\n",
    "            SaveObject(D,\"D\")\n",
    "            SaveObject(T,\"Time\")\n",
    "            SaveObject(epsilon,\"Epsilon\")\n",
    "\n",
    "            loss_df.to_csv(\"./Objects/loss_dataframe.csv\",index=False)\n",
    "            scores_df.to_csv(\"./Objects/scores_dataframe.csv\",index=False)\n",
    "            actions_df.to_csv(\"./Objects/actions_dataframe.csv\",index=False)\n",
    "            Qvalues_df.to_csv(\"./Objects/Qvalues_dataframe.csv\",index=False)\n",
    "\n",
    "            clear_output()\n",
    "            print ('Model Weights Saved')\n",
    "            Game_State._game.resume()\n",
    "\n",
    "        state = \"\"\n",
    "        if t<= Observe:\n",
    "            state = \"Observating\"\n",
    "        elif t>Observe and t<=Observe+Explore:\n",
    "            state = 'Explorating'\n",
    "        else:\n",
    "            state = 'Training'\n",
    "\n",
    "        print (\"Time-Step:\",t, \"/ State\", state)\n",
    "    print (\"Episode Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZxLeeUsWOW-c"
   },
   "outputs": [],
   "source": [
    "def playGame(ObservePerformance):\n",
    "    game = Game()\n",
    "    dino = DinoAgent(game)\n",
    "    game_state = Game_State(dino,game)    \n",
    "    model = BuildModel()\n",
    "    try:\n",
    "        TrainModel(model,game_state,ObservePerformance)\n",
    "    except StopIteration:\n",
    "        game.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "w-eJG5Dzasv9",
    "outputId": "35ba6b08-f373-49c4-89b4-5e713aca002a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fps: 2.2134289566660597\n",
      "Time-Step: 1 / State Observating\n",
      "Performing Random Action\n",
      "fps: 1.820899669361785\n",
      "Time-Step: 2 / State Observating\n",
      "Performing Random Action\n",
      "fps: 1.602499626530794\n",
      "Time-Step: 3 / State Observating\n",
      "fps: 1.4570780234088292\n",
      "Time-Step: 4 / State Observating\n",
      "fps: 1.3403359800440546\n",
      "Time-Step: 5 / State Observating\n",
      "fps: 1.2460851726909061\n",
      "Time-Step: 6 / State Observating\n",
      "fps: 1.1521407828275891\n",
      "Time-Step: 7 / State Observating\n",
      "Performing Random Action\n",
      "fps: 1.0656744806229754\n",
      "Time-Step: 8 / State Observating\n",
      "fps: 1.0040460554191337\n",
      "Time-Step: 9 / State Observating\n",
      "fps: 0.9417722041957244\n",
      "Time-Step: 10 / State Observating\n",
      "fps: 0.8891259342671094\n",
      "Time-Step: 11 / State Observating\n",
      "fps: 0.8429867141738525\n",
      "Time-Step: 12 / State Observating\n",
      "fps: 0.8029378426811509\n",
      "Time-Step: 13 / State Observating\n",
      "fps: 0.7660406677111143\n",
      "Time-Step: 14 / State Observating\n",
      "fps: 0.7325100149600048\n",
      "Time-Step: 15 / State Observating\n",
      "Performing Random Action\n",
      "fps: 0.6955631132169966\n",
      "Time-Step: 16 / State Observating\n",
      "fps: 0.6703657176101292\n",
      "Time-Step: 17 / State Observating\n",
      "fps: 0.64572010508741\n",
      "Time-Step: 18 / State Observating\n",
      "fps: 0.6233223974980933\n",
      "Time-Step: 19 / State Observating\n",
      "fps: 0.5993451121690105\n",
      "Time-Step: 20 / State Observating\n",
      "fps: 0.5799403841266584\n",
      "Time-Step: 21 / State Observating\n",
      "fps: 0.559116558555083\n",
      "Time-Step: 22 / State Observating\n",
      "Performing Random Action\n",
      "fps: 0.5383241978440613\n",
      "Time-Step: 23 / State Observating\n",
      "fps: 0.5075892649141948\n",
      "Time-Step: 24 / State Observating\n",
      "fps: 0.4930460159385498\n",
      "Time-Step: 25 / State Observating\n",
      "fps: 0.4784565294316134\n",
      "Time-Step: 26 / State Observating\n",
      "fps: 0.4642709410147448\n",
      "Time-Step: 27 / State Observating\n",
      "fps: 0.45069308649950757\n",
      "Time-Step: 28 / State Observating\n",
      "fps: 0.4381439212041587\n",
      "Time-Step: 29 / State Observating\n",
      "fps: 0.4271255363759019\n",
      "Time-Step: 30 / State Observating\n",
      "fps: 0.4160599400317964\n",
      "Time-Step: 31 / State Observating\n",
      "fps: 0.4060830196600032\n",
      "Time-Step: 32 / State Observating\n",
      "fps: 0.3961643791107104\n",
      "Time-Step: 33 / State Observating\n",
      "fps: 0.38768181782049876\n",
      "Time-Step: 34 / State Observating\n",
      "fps: 0.37200899923350766\n",
      "Time-Step: 35 / State Observating\n",
      "fps: 0.36413756700114425\n",
      "Time-Step: 36 / State Observating\n",
      "fps: 0.351153720925696\n",
      "Time-Step: 37 / State Observating\n",
      "fps: 0.34444420720340446\n",
      "Time-Step: 38 / State Observating\n",
      "fps: 0.3378432487113712\n",
      "Time-Step: 39 / State Observating\n",
      "fps: 0.3315564387285187\n",
      "Time-Step: 40 / State Observating\n",
      "Performing Random Action\n",
      "fps: 0.3291666244839683\n",
      "Time-Step: 41 / State Observating\n",
      "fps: 0.3233976591530865\n",
      "Time-Step: 42 / State Observating\n",
      "fps: 0.31696490242208925\n",
      "Time-Step: 43 / State Observating\n",
      "Performing Random Action\n",
      "fps: 0.30993798733735717\n",
      "Time-Step: 44 / State Observating\n",
      "fps: 0.3038695423363773\n",
      "Time-Step: 45 / State Observating\n",
      "fps: 0.2988702184943505\n",
      "Time-Step: 46 / State Observating\n",
      "fps: 0.29368337331334227\n",
      "Time-Step: 47 / State Observating\n",
      "fps: 0.2888819371236249\n",
      "Time-Step: 48 / State Observating\n",
      "fps: 0.2802085551879987\n",
      "Time-Step: 49 / State Observating\n",
      "fps: 0.2718785256488166\n",
      "Time-Step: 50 / State Observating\n",
      "fps: 0.26770288384592106\n",
      "Time-Step: 51 / State Observating\n",
      "fps: 0.2637284972947781\n",
      "Time-Step: 52 / State Observating\n",
      "fps: 0.2598380348797057\n",
      "Time-Step: 53 / State Observating\n",
      "fps: 0.2561354466253395\n",
      "Time-Step: 54 / State Observating\n",
      "Performing Random Action\n",
      "fps: 0.2513490605476016\n",
      "Time-Step: 55 / State Observating\n",
      "fps: 0.24684971614340165\n",
      "Time-Step: 56 / State Observating\n",
      "fps: 0.2428886810206909\n",
      "Time-Step: 57 / State Observating\n",
      "fps: 0.2392649064381725\n",
      "Time-Step: 58 / State Observating\n",
      "fps: 0.23605373328165366\n",
      "Time-Step: 59 / State Observating\n",
      "fps: 0.2330224425904373\n",
      "Time-Step: 60 / State Observating\n",
      "fps: 0.22971618634142146\n",
      "Time-Step: 61 / State Observating\n",
      "fps: 0.22384225398896251\n",
      "Time-Step: 62 / State Observating\n",
      "fps: 0.2211184206815194\n",
      "Time-Step: 63 / State Observating\n",
      "fps: 0.21830204546786483\n",
      "Time-Step: 64 / State Observating\n",
      "fps: 0.21563195763020895\n",
      "Time-Step: 65 / State Observating\n",
      "fps: 0.21253827147156173\n",
      "Time-Step: 66 / State Observating\n",
      "fps: 0.2099578693053431\n",
      "Time-Step: 67 / State Observating\n",
      "fps: 0.20730232531473172\n",
      "Time-Step: 68 / State Observating\n",
      "fps: 0.20460578435313354\n",
      "Time-Step: 69 / State Observating\n",
      "fps: 0.20196827266995754\n",
      "Time-Step: 70 / State Observating\n",
      "fps: 0.19971361220075068\n",
      "Time-Step: 71 / State Observating\n",
      "fps: 0.19720162310467548\n",
      "Time-Step: 72 / State Observating\n",
      "fps: 0.19507098692626126\n",
      "Time-Step: 73 / State Observating\n",
      "fps: 0.19293284928514912\n",
      "Time-Step: 74 / State Observating\n",
      "Performing Random Action\n",
      "fps: 0.19031716138887156\n",
      "Time-Step: 75 / State Observating\n",
      "fps: 0.18822638213962523\n",
      "Time-Step: 76 / State Observating\n",
      "fps: 0.18592251866060913\n",
      "Time-Step: 77 / State Observating\n",
      "fps: 0.18228752612641963\n",
      "Time-Step: 78 / State Observating\n",
      "fps: 0.1786259058890277\n",
      "Time-Step: 79 / State Observating\n",
      "fps: 0.17679201804378677\n",
      "Time-Step: 80 / State Observating\n",
      "fps: 0.17498087510879715\n",
      "Time-Step: 81 / State Observating\n",
      "Performing Random Action\n",
      "fps: 0.17428781226017165\n",
      "Time-Step: 82 / State Observating\n",
      "fps: 0.17252261860975276\n",
      "Time-Step: 83 / State Observating\n",
      "fps: 0.170777064421487\n",
      "Time-Step: 84 / State Observating\n",
      "fps: 0.16911045277231332\n",
      "Time-Step: 85 / State Observating\n",
      "fps: 0.16747414311181538\n",
      "Time-Step: 86 / State Observating\n",
      "fps: 0.16580121464107567\n",
      "Time-Step: 87 / State Observating\n",
      "fps: 0.16388215484334545\n",
      "Time-Step: 88 / State Observating\n",
      "fps: 0.1623692642782075\n",
      "Time-Step: 89 / State Observating\n",
      "fps: 0.16089601151431723\n",
      "Time-Step: 90 / State Observating\n",
      "fps: 0.15811256383935224\n",
      "Time-Step: 91 / State Observating\n",
      "fps: 0.15663429033284315\n",
      "Time-Step: 92 / State Observating\n",
      "fps: 0.15408814077183453\n",
      "Time-Step: 93 / State Observating\n",
      "fps: 0.15266009868174055\n",
      "Time-Step: 94 / State Observating\n",
      "fps: 0.15127759529482152\n",
      "Time-Step: 95 / State Observating\n",
      "fps: 0.14993687485922094\n",
      "Time-Step: 96 / State Observating\n",
      "fps: 0.14858803594492892\n",
      "Time-Step: 97 / State Observating\n",
      "fps: 0.14709398733252746\n",
      "Time-Step: 98 / State Observating\n",
      "Performing Random Action\n",
      "fps: 0.14553023815195962\n",
      "Time-Step: 99 / State Observating\n",
      "fps: 0.1443025010758255\n",
      "Time-Step: 100 / State Observating\n",
      "Performing Random Action\n",
      "fps: 0.1427201301850797\n",
      "Time-Step: 101 / State Explorating\n",
      "fps: 0.14143848156932864\n",
      "Time-Step: 102 / State Explorating\n",
      "fps: 0.1010034230069073\n",
      "Time-Step: 103 / State Explorating\n",
      "fps: 0.08118979452824705\n",
      "Time-Step: 104 / State Explorating\n",
      "fps: 0.06778901231068495\n",
      "Time-Step: 105 / State Explorating\n",
      "fps: 0.05789478965748245\n",
      "Time-Step: 106 / State Explorating\n",
      "fps: 0.05082801056365572\n",
      "Time-Step: 107 / State Explorating\n",
      "fps: 0.045177530734220185\n",
      "Time-Step: 108 / State Explorating\n",
      "fps: 0.040606033164166375\n",
      "Time-Step: 109 / State Explorating\n"
     ]
    },
    {
     "ename": "WebDriverException",
     "evalue": "Message: chrome not reachable\n  (Session info: chrome=85.0.4183.83)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-754ae3d0576a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplayGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-d7a4770f8d6c>\u001b[0m in \u001b[0;36mplayGame\u001b[0;34m(ObservePerformance)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBuildModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mTrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgame_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mObservePerformance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-5ca26cd42eb9>\u001b[0m in \u001b[0;36mTrainModel\u001b[0;34m(model, Game_State, ObservePerformance)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mepsilon\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mInitial_Epsilon\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mFinal_Epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExplore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mxt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtermination\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGame_State\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'fps: {0}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlastTime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-4b73532ab2f1>\u001b[0m in \u001b[0;36mget_state\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_game\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0misOver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-09e50ced21ff>\u001b[0m in \u001b[0;36mget_score\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_driver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_tag_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"body\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mARROW_DOWN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mscore_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_driver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"return Runner.instance_.distanceMeter.digits\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute_script\u001b[0;34m(self, script, *args)\u001b[0m\n\u001b[1;32m    634\u001b[0m         return self.execute(command, {\n\u001b[1;32m    635\u001b[0m             \u001b[0;34m'script'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m             'args': converted_args})['value']\n\u001b[0m\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexecute_async_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: chrome not reachable\n  (Session info: chrome=85.0.4183.83)\n"
     ]
    }
   ],
   "source": [
    "playGame(False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Dino-Game.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
